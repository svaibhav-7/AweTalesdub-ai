# Dubsmart AI üéôÔ∏è

Professional, high-performance multilingual audio dubbing system.

## üèóÔ∏è Project Structure
The project follows professional modular standards:

```
dubsmart-ai/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ dubsmart/       # Core package
‚îÇ       ‚îú‚îÄ‚îÄ core/       # Orchestration pipeline
‚îÇ       ‚îú‚îÄ‚îÄ modules/    # AI engines (Whisper, M2M100, XTTS)
‚îÇ       ‚îú‚îÄ‚îÄ processor/  # Audio processing and mixing
‚îÇ       ‚îú‚îÄ‚îÄ api/        # FastAPI modular backend
‚îÇ       ‚îî‚îÄ‚îÄ utils/      # Shared utilities and config
‚îú‚îÄ‚îÄ webapp/             # Premium React frontend
‚îú‚îÄ‚îÄ scripts/            # Utility and standalone scripts
‚îú‚îÄ‚îÄ setup.py            # Package installation
‚îî‚îÄ‚îÄ requirements.txt    # Project dependencies
```

## üöÄ Installation Guide (Recommended)

To ensure a clean environment, we highly recommend using a Python Virtual Environment (`.venv`).

### 1. Set up Virtual Environment
**Windows:**
```powershell
# Create venv if not exists
python -m venv .venv
# Activate venv
.\.venv\Scripts\activate
```

**macOS/Linux:**
```bash
# Create venv if not exists
python3 -m venv .venv
# Activate venv
source .venv/bin/activate
```

### 2. Install Project Dependencies
Once your `.venv` is active, install the project in **editable mode**:
```bash
pip install -e .
```
> [!NOTE]
> This will register the `dubsmart` command globally within your environment.

## üéôÔ∏è Usage

### Command Line Interface (CLI)
After installation, you can run the dubber from anywhere inside your environment:

```bash
# Dub from English to Hindi
dubsmart --input test_audio/English.wav --tgt hi --output result.wav
```

### üåç Supported Languages
The system currently supports seamless cross-language dubbing between:
- English (`en`)
- Hindi (`hi`)
- Telugu (`te`)
- Spanish (`es`)

> [!TIP]
> **Same-Language Dubbing**: If you set the target language to be the same as the input language, the system will re-synthesize the audio using the cloned voice of the original speaker. This is great for voice cleaning or "voice matching" tests!

### ‚ûï Adding New Languages
The architecture is designed to be extensible. To add a new language:
1. Update `SUPPORTED_LANGUAGES` in `src/dubsmart/utils/config.py`.
2. The internal M2M100 translation model already supports **100+ languages**, so it will likely work immediately!

### Key Optimizations
- **Lazy Loading**: Heavy AI models (Whisper, XTTS) are only loaded into memory when synthesis starts, ensuring an near-instant CLI startup.
- **Zero-Shot Voice Cloning**: High-quality cloning using the Coqui XTTS-v2 model.
- **Unified Pipeline**: Robust error handling and centralized logging in `logs/dubsmart.log`.

## üéØ Features

‚úÖ **Auto-detects source language** (English / Hindi / Telugu)  
‚úÖ **Rejects same-language dubbing** (source ‚â† target validation)  
‚úÖ **Auto-assigns speaker IDs** (speaker diarization)  
‚úÖ **Assigns separate voices per speaker**  
‚úÖ **Translates to target language**  
‚úÖ **Includes noise suppression**  
‚úÖ **Preserves timing and emotion**  
‚úÖ **Hackathon-realistic** (no training required, uses pre-trained models)

## üìã Prerequisites

### System Requirements

- **Python 3.8+**
- **FFmpeg** (must be installed system-wide)

#### Install FFmpeg

**Windows:**
```powershell
# Using Chocolatey
choco install ffmpeg

# Or download from https://ffmpeg.org/download.html
```
**Linux:**
```bash
sudo apt-get install ffmpeg
```
**macOS:**
```bash
brew install ffmpeg
```
### Python Dependencies

Install all required packages:

```bash
pip install -r requirements.txt
```
**Note:** The first time you run Whisper, it will download model files (~1-3 GB depending on model size).

## üöÄ Quick Start
### Basic Usage (Package installed)

```bash
dubsmart --input test_audio/English.wav --tgt es
```

### Command-Line Options
```bash
python -m dubsmart.main --help

Arguments:
  input             Input audio file (WAV, MP3, etc.)
  tgt               Target language: en, hi, es, fr, etc.
  output            Output audio file
```

### Examples

**English to Spanish:**
```bash
dubsmart --input test_audio/English.wav --tgt es --output result_es.wav
```

**With background preservation:**
```bash
python audio_dubbing.py video_audio.wav te dubbed_telugu.wav --preserve-background
```

## üíª Python API Usage

```python
from audio_dubbing import AudioDubber

# Create dubber instance
dubber = AudioDubber()

# Dub audio
results = dubber.dub_audio(
    input_file="sample.wav",
    target_language="hi",  # Hindi
    output_file="output.wav"
)

if results['status'] == 'success':
    print(f"Dubbed successfully: {results['output_file']}")
    print(f"Detected language: {results['detected_language_name']}")
    print(f"Number of speakers: {results['num_speakers']}")
```

See [`example_usage.py`](example_usage.py) for more examples.

## üîÅ How It Works - Complete Pipeline

```
Input Audio
    ‚Üì
Audio Extraction & Conversion (mono, 16kHz)
    ‚Üì
Noise Suppression (FFmpeg afftdn filter)
    ‚Üì
Voice Activity Detection (Silero VAD)
    ‚Üì
Speaker Diarization (PyAnnote or energy-based fallback)
    ‚Üì
Speech-to-Text (Whisper - multilingual)
    ‚Üì
Automatic Language Detection (Whisper)
    ‚Üì
Source ‚â† Target Validation
    ‚Üì
Text Segmentation & Alignment (merge speaker segments)
    ‚Üì
Translation (MarianMT - source ‚Üí target)
    ‚Üì
Voice Assignment (unique voice per speaker)
    ‚Üì
Emotion-aware Text-to-Speech (gTTS with prosody)
    ‚Üì
Timing Alignment (match original duration)
    ‚Üì
Audio Mixing (combine all segments)
    ‚Üì
Final Dubbed Audio
```

## üß© Project Structure

```
dubsmart-ai/
‚îú‚îÄ‚îÄ audio_dubbing.py          # Main pipeline orchestrator
‚îú‚îÄ‚îÄ audio_processor.py         # Audio preprocessing (noise, VAD)
‚îú‚îÄ‚îÄ speaker_diarization.py     # Speaker identification
‚îú‚îÄ‚îÄ transcription.py           # Speech-to-text (Whisper)
‚îú‚îÄ‚îÄ translation.py             # Translation (MarianMT)
‚îú‚îÄ‚îÄ voice_synthesis.py         # Text-to-speech (gTTS)
‚îú‚îÄ‚îÄ audio_mixer.py             # Audio mixing
‚îú‚îÄ‚îÄ config.py                  # Configuration settings
‚îú‚îÄ‚îÄ utils.py                   # Utility functions
‚îú‚îÄ‚îÄ example_usage.py           # Usage examples
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îú‚îÄ‚îÄ README.md                  # This file
‚îú‚îÄ‚îÄ temp/                      # Temporary processing files
‚îú‚îÄ‚îÄ output/                    # Output directory
‚îî‚îÄ‚îÄ models/                    # Downloaded models cache
```

## ‚öôÔ∏è Configuration

Edit [`config.py`](config.py) to customize:

- **Whisper Model**: Change `WHISPER_MODEL` (options: `tiny`, `base`, `small`, `medium`, `large`)
- **Translation Method**: Change `TRANSLATION_METHOD` (`marian`, `google`, `gemini`)
- **TTS Engine**: Change `TTS_ENGINE` (`gtts`, `pyttsx3`, `coqui`)
- **Voice Mappings**: Customize voices per speaker per language
- **Audio Settings**: Sample rate, noise reduction parameters, etc.

### Using PyAnnote (Better Speaker Diarization)

PyAnnote provides better speaker diarization but requires a HuggingFace token:

1. Get a token from [HuggingFace](https://huggingface.co/settings/tokens)
2. Accept the [PyAnnote model terms](https://huggingface.co/pyannote/speaker-diarization)
3. Set environment variable:
   ```bash
   # Windows PowerShell
   $env:HUGGINGFACE_TOKEN="your_token_here"
   
   # Linux/Mac
   export HUGGINGFACE_TOKEN="your_token_here"
   ```
4. Run with `--use-pyannote` flag

## üé§ Supported Languages

| Code | Language |
|------|----------|
| `en` | English  |
| `hi` | Hindi    |
| `te` | Telugu   |

> **Note:** You can extend language support by modifying `config.py` and ensuring the translation models support your language pairs.

## üîß Troubleshooting

### "FFmpeg not found"
- Install FFmpeg system-wide (see Prerequisites section)
- Ensure it's in your PATH

### "Model download failed"
- Check internet connection (Whisper downloads ~1-3GB on first run)
- Models are cached in `~/.cache/whisper/`

### "PyAnnote authentication failed"
- Set `HUGGINGFACE_TOKEN` environment variable
- Accept model terms at https://huggingface.co/pyannote/speaker-diarization

### "Translation quality is poor"
- MarianMT has limited support for some Indic language pairs
- Consider using Google Translate API (requires API key in `config.py`)

### "Speaker diarization not working well"
- Try with `--use-pyannote` for better quality
- Or use audio with clearly distinct speakers
- Simple fallback works best with 1-2 speakers

## üìä Performance Tips

1. **Use smaller Whisper models** for faster processing: `WHISPER_MODEL = 'tiny'` or `'base'`
2. **Disable intermediate file saving** with `--no-intermediates`
3. **Use GPU** if available (automatically detected for Whisper and PyAnnote)
4. **Process shorter clips** (1-2 minutes recommended for demos)

## üéì Hackathon Demo Tips

1. **Prepare 2-3 sample audio files** with different scenarios:
   - Single speaker (news anchor)
   - Two speakers (interview/conversation)
   - With background music/noise

2. **Show intermediate outputs**:
   - `temp/speaker_segments.json` - Speaker diarization results
   - `temp/transcription.json` - Original transcription
   - `temp/translated_segments.json` - Translated text

3. **Highlight key features**:
   - Language auto-detection
   - Same-language rejection
   - Multiple speakers with different voices
   - Timing preservation

4. **Be ready to explain each step** using the pipeline diagram

## üìù License

This project is for educational and hackathon purposes.

## üôè Acknowledgments

- **OpenAI Whisper** - Multilingual speech recognition
- **PyAnnote** - Speaker diarization
- **MarianMT** - Neural machine translation
- **gTTS** - Text-to-speech synthesis
- **FFmpeg** - Audio processing

---

**Made for hackathons | No training required | Uses only pre-trained models**
